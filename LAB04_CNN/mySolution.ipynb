{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on Convolutional Neural Network\n",
    "The main goal of this laboratory is to solve a multiclass classification problem with 10 different classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "num_classes = 10\n",
    "new_im_size = 32\n",
    "channels = 3\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_learn, y_learn),(x_test, y_test) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing training set..\n",
      "Normalizing test set..\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data in [0 1]\n",
    "print(\"Normalizing training set..\")\n",
    "x_learn = np.asarray(x_learn, dtype=np.float32) / 255 # Normalizing training set\n",
    "print(\"Normalizing test set..\")\n",
    "x_test = np.asarray(x_test, dtype=np.float32) / 255 # Normalizing test set\n",
    "# split in training and validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_learn, y_learn, test_size=0.25, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing training set..\n",
      "Standardizing validation set..\n",
      "Standardizing test set..\n",
      "Size of the training set\n",
      "x_train (37500, 32, 32, 3)\n",
      "y_train (37500, 1)\n",
      "Size of the validation set\n",
      "x_val (12500, 32, 32, 3)\n",
      "y_val (12500, 1)\n",
      "Size of the test set\n",
      "x_test (10000, 32, 32, 3)\n",
      "y_test (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Standardizing the data\n",
    "# This process is useful to make the model converge faster\n",
    "\n",
    "def standardize_dataset(X):\n",
    "    image_means = []\n",
    "    image_stds = []\n",
    "\n",
    "    for image in X:\n",
    "        image_means.append(np.mean(image)) # Computing the image mean\n",
    "        image_stds.append(np.std(image)) # Computing the image standard deviation\n",
    "\n",
    "    dataset_mean = np.mean(image_means) # Computing the dataset mean\n",
    "    dataset_std = np.mean(image_stds) # Computing the dataset standard deviation\n",
    "    return [dataset_mean, dataset_std] # For every image we subtract to it the dataset mean and we divide by the dataset standard deviation\n",
    "\n",
    "dataset_mean, dataset_std = standardize_dataset(x_train)\n",
    "\n",
    "print(\"Standardizing training set..\")\n",
    "x_train = (x_train-dataset_mean)/dataset_std # Standardizing the training set\n",
    "print(\"Standardizing validation set..\")\n",
    "x_val = (x_val-dataset_mean)/dataset_std # Standardizing the test set\n",
    "print(\"Standardizing test set..\")\n",
    "x_test = (x_test-dataset_mean)/dataset_std # Standardizing the test set\n",
    "\n",
    "# one hot encode target values\n",
    "y_train_enc = tf.keras.utils.to_categorical(y_train)\n",
    "y_val_enc = tf.keras.utils.to_categorical(y_val)\n",
    "y_test_enc = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "print(\"Size of the training set\")\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "\n",
    "print(\"Size of the validation set\")\n",
    "print(\"x_val\", x_val.shape)\n",
    "print(\"y_val\", y_val.shape)\n",
    "\n",
    "print(\"Size of the test set\")\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model from scratch\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import Sequential,Input,Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 1024)      28672     \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 32, 32, 1024)      0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 16, 16, 1024)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16, 16, 1024)      0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 254)       2341118   \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16, 16, 254)       0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 8, 8, 254)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 8, 8, 254)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 16256)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                1040448   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3410888 (13.01 MB)\n",
      "Trainable params: 3410888 (13.01 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "scratch_model = Sequential()\n",
    "\n",
    "# Build here your keras model.\n",
    "# Try to use one convolutional layer, joint with pooling layer and dropout layer\n",
    "\n",
    "# Creating conv 1: conv with 1024 kernels of size 3x3, padding='same', input_shape=(new_im_size, new_im_size, channels)\n",
    "# + LeakyReLU(alpha=0.1) + maxpooling with region size 2x2 and padding='same'+Dropout(0.25)\n",
    "scratch_model.add(Conv2D(1024, kernel_size=(3, 3), padding='same',input_shape=(new_im_size, new_im_size, channels)))\n",
    "scratch_model.add(LeakyReLU(alpha=0.1))\n",
    "scratch_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "scratch_model.add(Dropout(0.25))\n",
    "\n",
    "scratch_model.add(Conv2D(254, kernel_size=(3, 3), padding='same',input_shape=(new_im_size, new_im_size, channels)))\n",
    "scratch_model.add(LeakyReLU(alpha=0.1))\n",
    "scratch_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "scratch_model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# Adding the dense final part: Flatten + Dense with 64 neurons and relu + Dropout 25% + Dense with 10 neurons and softmax\n",
    "scratch_model.add(Flatten())\n",
    "scratch_model.add(Dense(64, activation='relu'))\n",
    "scratch_model.add(Dropout(0.25))\n",
    "scratch_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model with the Adam optimizer\n",
    "scratch_model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "\n",
    "# Visualize the model through the summary function\n",
    "scratch_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 32, 32, 1024)      28672     \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 1024)      0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 16, 16, 1024)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 16, 16, 1024)      0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 254)       2341118   \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 16, 16, 254)       0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 8, 8, 254)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 8, 8, 254)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 8, 8, 128)         292736    \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 4, 4, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 64)          73792     \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 2, 2, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 2, 2, 64)          0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2753416 (10.50 MB)\n",
      "Trainable params: 2753416 (10.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating another network deeper, adding more Conv and Polling layers\n",
    "\n",
    "deep_model = Sequential()\n",
    "\n",
    "deep_model.add(Conv2D(1024, kernel_size=(3, 3), padding='same',input_shape=(new_im_size, new_im_size, channels)))\n",
    "deep_model.add(LeakyReLU(alpha=0.1))\n",
    "deep_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "deep_model.add(Dropout(0.25))\n",
    "\n",
    "deep_model.add(Conv2D(254, kernel_size=(3, 3), padding='same',input_shape=(new_im_size, new_im_size, channels)))\n",
    "deep_model.add(LeakyReLU(alpha=0.1))\n",
    "deep_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "deep_model.add(Dropout(0.25))\n",
    "\n",
    "deep_model.add(Conv2D(128, kernel_size=(3, 3), padding='same',input_shape=(new_im_size, new_im_size, channels)))\n",
    "deep_model.add(LeakyReLU(alpha=0.1))\n",
    "deep_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "deep_model.add(Dropout(0.25))\n",
    "\n",
    "deep_model.add(Conv2D(64, kernel_size=(3, 3), padding='same',input_shape=(new_im_size, new_im_size, channels)))\n",
    "deep_model.add(LeakyReLU(alpha=0.1))\n",
    "deep_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "deep_model.add(Dropout(0.25))\n",
    "\n",
    "deep_model.add(Flatten())\n",
    "deep_model.add(Dense(64, activation='relu'))\n",
    "deep_model.add(Dropout(0.25))\n",
    "deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "               \n",
    "deep_model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "\n",
    "# Visualize the model through the summary function\n",
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 84/586 [===>..........................] - ETA: 13:18 - loss: 2.0514 - accuracy: 0.2679"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "scratch_model_history = scratch_model.fit(x_train, y_train_enc, batch_size=batch_size, shuffle=True, epochs=epochs, validation_data=(x_val, y_val_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model_history = deep_model.fit(x_train, y_train_enc, batch_size=batch_size, shuffle=True, epochs=epochs, validation_data=(x_val, y_val_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the results\n",
    "\n",
    "plot_history(scratch_model_history)\n",
    "\n",
    "print(\"Training accuracy: \", accuracy_score(np.argmax(scratch_model.predict(x_train), axis=-1), y_train))\n",
    "print(\"Validation accuracy: \", accuracy_score(np.argmax(scratch_model.predict(x_val), axis=-1), y_val))\n",
    "print(\"Test accuracy: \", accuracy_score(np.argmax(scratch_model.predict(x_test), axis=-1), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(deep_model_history)\n",
    "\n",
    "print(\"Training accuracy: \", accuracy_score(np.argmax(deep_model.predict(x_train), axis=-1), y_train))\n",
    "print(\"Validation accuracy: \", accuracy_score(np.argmax(deep_model.predict(x_val), axis=-1), y_val))\n",
    "print(\"Test accuracy: \", accuracy_score(np.argmax(deep_model.predict(x_test), axis=-1), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding data augmentation\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([tensorflow.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "tensorflow.keras.layers.experimental.preprocessing.RandomRotation(0.2)])\n",
    "\n",
    "model.add(data_augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained model ( Xception Net model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model based over the pretrained Xception network\n",
    "from tensorflow.keras import applications\n",
    "import tensorflow\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(tensorflow.keras.layers.UpSampling2D(size=(7,7),input_shape=(32,32,3)))\n",
    "\n",
    "Xception_model = applications.Xception(weights = \"imagenet\", include_top=False, input_shape = (224, 224, channels))\n",
    "\n",
    "for layer in Xception_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "Inputs = layers.Input(shape=(32,32,3))\n",
    "x = model(Inputs)\n",
    "x = Xception_model(x)\n",
    "x = layers.Flatten()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "# and a logistic layer for 10 classes\n",
    "predictions = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "pre_trained_model = tensorflow.keras.Model(Inputs, outputs=predictions)\n",
    "pre_trained_model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.Adam(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model through the summary function\n",
    "pre_trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the model!\n",
    "epochs = 5 #it may take a while, the number of epochs should be low...\n",
    "pretrained_model_history = pre_trained_model.fit(x_train, y_train_enc, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the results\n",
    "plot_history(pretrained_model_history)\n",
    "\n",
    "print(\"Training accuracy: \", accuracy_score(np.argmax(pre_trained_model.predict(x_train), axis=-1), y_train))\n",
    "print(\"Validation accuracy: \", accuracy_score(np.argmax(pre_trained_model.predict(x_val), axis=-1), y_val))\n",
    "print(\"Test accuracy: \", accuracy_score(np.argmax(pre_trained_model.predict(x_test), axis=-1), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create here the plots to compare the \"from scratch\" model and the \"pretrained\" model\n",
    "# Try to produce a comparison plot about the accuracies (train and validation) and another plot for the losses\n",
    "# Creating the plots to compare the \"from scratch\" model and the \"pretrained\" model\n",
    "# Producing accuracy over epochs plot\n",
    "\n",
    "scratch_model_train_acc = scratch_model_history.history['accuracy']\n",
    "scratch_model_valid_acc = scratch_model_history.history['val_accuracy']\n",
    "scratch_model_train_loss = scratch_model_history.history['loss']\n",
    "scratch_model_valid_loss = scratch_model_history.history['val_loss']\n",
    "\n",
    "pretrained_model_train_acc = pretrained_model_history.history['accuracy']\n",
    "pretrained_model_valid_acc = pretrained_model_history.history['val_accuracy']\n",
    "pretrained_model_train_loss = pretrained_model_history.history['loss']\n",
    "pretrained_model_valid_loss = pretrained_model_history.history['val_loss']\n",
    "\n",
    "print(\"Producing accuracy over epochs plot\")\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(16,7))\n",
    "\n",
    "plt.plot(scratch_model_train_acc, label=\"Scratch Train Acc.\")\n",
    "plt.plot(scratch_model_valid_acc, label=\"Scratch Valid. Acc.\")\n",
    "\n",
    "plt.plot(pretrained_model_train_acc, label=\"Pre-Trained Train Acc.\")\n",
    "plt.plot(pretrained_model_valid_acc, label=\"Pre-Trained Valid. Acc.\")\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy(%)')\n",
    "plt.legend(loc='lower right', fancybox=True, shadow=True, ncol=4)\n",
    "plt.grid()\n",
    "plt.savefig('acc_epochs.png', dpi=300)\n",
    "\n",
    "\n",
    "# Producing loss over epochs plot\n",
    "print(\"Producing loss over epochs plot\")\n",
    "fig = plt.figure(figsize=(16,7))\n",
    "\n",
    "plt.plot(scratch_model_train_loss, label=\"Scratch Train Loss\")\n",
    "plt.plot(scratch_model_valid_loss, label=\"Scratch Valid. Loss\")\n",
    "\n",
    "plt.plot(pretrained_model_train_loss, label=\"Pre-Trained Train Loss\")\n",
    "plt.plot(pretrained_model_valid_loss, label=\"Pre-Trained Valid. Loss\")\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right', fancybox=True, shadow=True, ncol=4)\n",
    "plt.grid()\n",
    "plt.savefig('loss_epochs.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
